# -*- coding: utf-8 -*-
"""amazon_review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AWQFRmNEqu6v2A1tbSkxC14FiIY22Lu7
"""

import warnings 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns

import re
import json 
import nltk
import spacy
import string
import unicodedata
from bs4 import BeautifulSoup
from textblob import TextBlob 
from nltk.stem import WordNetLemmatizer

from IPython import display 
display.set_matplotlib_formats('svg')
warnings.filterwarnings('ignore')

!pip install contractions
!pip install textsearch
!pip install tqdm
import nltk
nltk.download('punkt')

import pandas as pd
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.layers import MaxPooling1D
from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from sklearn.preprocessing import LabelEncoder

# fix random seed for reproducibility
seed = 42

data = pd.read_csv("/content/drive/MyDrive/datafiniti-consumer-reviews-of-amazon-products/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv")
data.head()

reviews = data['reviews.text'].values
sentiments = data['reviews.rating'].values

train_reviews = reviews[:19000]
train_sentiments = sentiments[:19000]

test_reviews = reviews[19000:]
test_sentiments = sentiments[19000:]

import contractions
from bs4 import BeautifulSoup
import numpy as np
import re
import tqdm
import unicodedata


def strip_html_tags(text):
  soup = BeautifulSoup(text, "html.parser")
  [s.extract() for s in soup(['iframe', 'script'])]
  stripped_text = soup.get_text()
  stripped_text = re.sub(r'[\r|\n|\r\n]+', '\n', stripped_text)
  return stripped_text

def remove_accented_chars(text):
  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
  return text

def pre_process_corpus(docs):
  norm_docs = []
  for doc in tqdm.tqdm(docs):
    doc = strip_html_tags(doc)
    doc = doc.translate(doc.maketrans("\n\t\r", "   "))
    doc = doc.lower()
    doc = remove_accented_chars(doc)
    doc = contractions.fix(doc)
    # lower case and remove special characters\whitespaces
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc, re.I|re.A)
    doc = re.sub(' +', ' ', doc)
    doc = doc.strip()  
    norm_docs.append(doc)
  
  return norm_docs

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# norm_train_reviews = pre_process_corpus(train_reviews)
# norm_test_reviews = pre_process_corpus(test_reviews)

t = Tokenizer(oov_token='<UNK>')
# fit the tokenizer on the documents
t.fit_on_texts(norm_train_reviews)
t.word_index['<PAD>'] = 0

max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), t.word_index['<UNK>']

train_sequences = t.texts_to_sequences(norm_train_reviews)

test_sequences = t.texts_to_sequences(norm_test_reviews)

print("Vocabulary size={}".format(len(t.word_index)))
print("Number of Documents={}".format(t.document_count))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

train_lens = [len(s) for s in train_sequences]
test_lens = [len(s) for s in test_sequences]

fig, ax = plt.subplots(1,2, figsize=(12, 6))
h1 = ax[0].hist(train_lens)
h2 = ax[1].hist(test_lens)

MAX_SEQUENCE_LENGTH = 1000

# pad dataset to a maximum review length in words
X_train = sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)
X_test = sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)
X_train.shape, X_test.shape

le = LabelEncoder()
num_classes=5 # positive -> 1, negative -> 0

y_train = le.fit_transform(train_sentiments)
y_test = le.transform(test_sentiments)

y_test.shape

yt=y_test

import tensorflow as tf
y_train=tf.keras.utils.to_categorical(y_train, num_classes)

y_test=tf.keras.utils.to_categorical(y_test, num_classes)

from sklearn.preprocessing import LabelEncoder,OneHotEncoder

le=OneHotEncoder()

temp=le.fit_transform(ytrain)
ytrain=temp.reshape(-1,1)

temp=le.fit_transform(ytest)
ytest=temp.reshape(-1,1)


print(xtrain_tf.shape,xtest_tf.shape)
print(ytrain.shape,ytest.shape)

VOCAB_SIZE = len(t.word_index)

EMBED_SIZE = 300
EPOCHS=2
BATCH_SIZE=128

# create the model
model = Sequential()
model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=MAX_SEQUENCE_LENGTH))
model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(5, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Fit the model
model.fit(X_train, y_train, 
          validation_split=0.1,
          epochs=30, 
         
          verbose=1)

# Final evaluation of the model
scores = model.evaluate(X_test, y_test, verbose=1)
print("Accuracy: %.2f%%" % (scores[1]*100))

Y_pred=np.argmax(model.predict(X_test),axis=-1)

#Y_pred = (Y_pred > 0.5)*1 #greater than 0.50 on scale 0 to 1
Y_pred

import seaborn as sns
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,f1_score,ConfusionMatrixDisplay
plt.figure(figsize=(10,8),dpi=80)
cm=(confusion_matrix(yt,Y_pred))
#plt.title('1 signifies dog sounds and 0 signifies cat sounds \n'+'Accuracy:'+str(accuracy_score(y_test,Y_pred)))

disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[1,2,3,4,5])
disp.plot()











data = pd.read_csv("/content/drive/MyDrive/datafiniti-consumer-reviews-of-amazon-products/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv")
data.head()

data = data[['reviews.text', 'reviews.rating']]
data.head()

data.tail()

data.shape

data.isnull().sum()

data['reviews.rating'].value_counts()

data['reviews.text'] = data['reviews.text'].str.lower() 
data['reviews.text'] = data['reviews.text'].apply(lambda x: re.sub('[^a-z A-Z 0-9-]+', '', x))  # it removes the punctuation 
data.head()

from spacy.lang.en.stop_words import STOP_WORDS
data['reviews.text'] = data['reviews.text'].apply(lambda x: " ".join([i for i in x.split() if i not in STOP_WORDS]) )

data.head()

data['reviews.text'] = data['reviews.text'].apply(lambda x: re.sub(r'(http|https|ftp|ssh)://([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:/~+#-]*[\w@?^=%&/~+#-])?', '' , str(x)))
                                              
data.head()

data['reviews.text'] = data['reviews.text'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())

data['reviews.text'] = data['reviews.text'].apply(lambda x: " ".join(x.split()))

# Commented out IPython magic to ensure Python compatibility.
# %time
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
lemmatizer = WordNetLemmatizer()
def lemmatize_words(text):
    return " ".join([lemmatizer.lemmatize(word) for word in text.split()])

data["reviews.text"] = data["reviews.text"].apply(lambda text: lemmatize_words(text))
data.head()

data.tail()

from sklearn.model_selection import train_test_split 
xtrain, xtest, ytrain, ytest = train_test_split(data['reviews.text'], data['reviews.rating'], test_size = 0.3)

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()

# let's convert 
xtrain_bow = vectorizer.fit_transform(xtrain).toarray()
xtest_bow = vectorizer.transform(xtest).toarray()



# Convert text to numbers using (TF-IDF)
from sklearn.feature_extraction.text import TfidfVectorizer  

tf_vectorizer = TfidfVectorizer()

# let's convert 
xtrain_tf = tf_vectorizer.fit_transform(xtrain).toarray()
xtest_tf = tf_vectorizer.transform(xtest).toarray()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten
from tensorflow.keras.initializers import RandomNormal
def get_model(n_inputs, n_outputs):
  batch_size = 256  
  hidden_units = 64
  dropout = 0.2

  model = Sequential()
  model.add(Dense(hidden_units, input_dim=n_inputs,activation='relu',
          kernel_initializer='he_uniform'))
  model.add(Dropout(dropout))
  model.add(Dense(64,activation='relu',
          kernel_initializer='he_uniform'))
  model.add(Flatten())
  model.add(Dense(64,activation='relu'))

  model.add(Dropout(dropout))

  model.add(Dense(5,activation='softmax'))
  #model.Flat

  #model.add(Activation('softmax'))
  model.compile(loss='binary_crossentropy', optimizer='adam')
  return model

from sklearn.preprocessing import LabelEncoder,OneHotEncoder

le=LabelEncoder()

temp=le.fit_transform(ytrain)
ytrain=temp.reshape(-1,1)

temp=le.fit_transform(ytest)
ytest=temp.reshape(-1,1)


print(xtrain_tf.shape,xtest_tf.shape)
print(ytrain.shape,ytest.shape)

y_train = np.asarray(ytrain).astype('float32').reshape((-1,1))
y_test = np.asarray(ytest).astype('float32').reshape((-1,1))

y_train.shape

n_inputs= xtrain_tf.shape[1]
n_outputs=5
model = get_model(n_inputs, n_outputs)

model.summary()

model.fit(xtrain_tf,ytrain,verbose = 0,epochs = 50)

